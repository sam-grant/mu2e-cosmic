{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3f9129-03e7-454d-b579-409796461fe9",
   "metadata": {},
   "source": [
    "## CE-like background: second pass\n",
    "\n",
    "Scale up using Skeleton processor class\n",
    "\n",
    "Dataset: `nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.root`\n",
    "\n",
    "First file: `nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.001202_00050440.root`\n",
    "\n",
    "File lists: `/exp/mu2e/data/users/sgrant/mu2e_cosmic_ana/nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.*.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac514c9f-c3b2-443c-877c-cd28866caab6",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "A bit awkard but I'm using my dev version of pyutils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ae7a091-d544-49bf-b43e-c12489025eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import hist\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../../EventNtuple/utils/pyutils\")\n",
    "from pyprocess import Processor, Skeleton\n",
    "from pyplot import Plot\n",
    "from pyprint import Print\n",
    "from pyselect import Select\n",
    "from pyvector import Vector\n",
    "\n",
    "sys.path.append(\"../common\")\n",
    "from cut_manager import CutManager\n",
    "from analyse import Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed1eece-1c03-4931-a646-2fd9a8fb3250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to think about the class structure here \n",
    "# It's mostly about naming conventions to be honest \n",
    "# I want import_dataset to be called process_dataset, but then that makes it sound like it should be in processor, which complicates things quite a bi\n",
    "# I guess we could merge Importer and Processor? \n",
    "# How would this handle the single file case?\n",
    "# process_file which would either get the array or do some custom some processing \n",
    "# process_dataset which would either parallelise getting the array \n",
    "# need to think about whether it's worth it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95213526-3592-4178-b0bd-d685079d8049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f6a93d4-9902-4eb2-a952-48fc2994a929",
   "metadata": {},
   "source": [
    "# Define analysis methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19656203-ed82-4ddd-9813-3335dd55f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analyse:\n",
    "    \"\"\"Class to handle analysis functions\n",
    "    \n",
    "    This class encapsulates all the analysis-specific logic,\n",
    "    allowing it to be reused across different processors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, verbosity=1):\n",
    "        \"\"\"Initialise the analysis handler\n",
    "        \n",
    "        Args:\n",
    "            verbosity (int, optional): Level of output detail (0: minimal, 1: normal, 2: detailed)\n",
    "        \"\"\"\n",
    "        self.verbosity = verbosity\n",
    "        self.print_prefix = \"[Analyse] \"\n",
    "        \n",
    "        # Initialise tools\n",
    "        self.selector = Select(verbosity=self.verbosity)\n",
    "        self.vector = Vector(verbosity=self.verbosity) \n",
    "        \n",
    "        # Analysis configuration\n",
    "        self.on_spill = False  # Default to off-spill analysis\n",
    "        \n",
    "        print(f\"{self.print_prefix}Initialised\")\n",
    "\n",
    "    # Can't we use lazy initialisation or something? \n",
    "    # # I guess we want to make sure each instance is seperate, but this feels off\n",
    "    # def create_cut_manager(self):\n",
    "    #     \"\"\"Create a new CutManager instance\n",
    "        \n",
    "    #     Args:\n",
    "    #         verbosity (int, optional): Verbosity level for the cut manager\n",
    "        \n",
    "    #     Returns:\n",
    "    #         CutManager: A new cut manager instance\n",
    "    #     \"\"\"\n",
    "    #     return CutManager(verbosity=self.verbosity)\n",
    "    \n",
    "    def define_cuts(self, data, cut_manager, on_spill=None):\n",
    "        \"\"\"Define analysis cuts\n",
    "        \n",
    "        Args:\n",
    "            data: The data to apply cuts to\n",
    "            cut_manager: The CutManager instance to use\n",
    "            on_spill (bool, optional): Whether to apply on-spill specific cuts\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (modified data with additional fields, cut_manager with cuts defined)\n",
    "        \"\"\"\n",
    "        if on_spill is None:\n",
    "            on_spill = self.on_spill\n",
    "            \n",
    "        selector = self.selector\n",
    "        \n",
    "        # Tracker surfaces\n",
    "        trk_front = selector.select_surface(data[\"trkfit\"], sid=0)\n",
    "        trk_mid = selector.select_surface(data[\"trkfit\"], sid=1)\n",
    "        trk_back = selector.select_surface(data[\"trkfit\"], sid=2)\n",
    "        in_trk = (trk_front | trk_mid | trk_back)\n",
    "        \n",
    "        # 1. Truth track parent is electron \n",
    "        is_electron = data[\"trkmc\"][\"trkmcsim\"][\"pdg\"] == 11\n",
    "        is_trk_parent = data[\"trkmc\"][\"trkmcsim\"][\"nhits\"] == ak.max(data[\"trkmc\"][\"trkmcsim\"][\"nhits\"], axis=-1)\n",
    "        is_trk_parent_electron = is_electron & is_trk_parent \n",
    "        has_trk_parent_electron = ak.any(is_trk_parent_electron, axis=-1) # Any tracks with electron parents?\n",
    "    \n",
    "        cut_manager.add_cut(\n",
    "            name=\"is_truth_electron\", \n",
    "            description=\"Track parents are electrons (truth PID)\", \n",
    "            mask=has_trk_parent_electron # has_trk_parent_electron\n",
    "        )\n",
    "\n",
    "        # 1.5 Also include trk.pdg == 11\n",
    "        is_trk_electron = selector.is_electron(data[\"trk\"])\n",
    "    \n",
    "        cut_manager.add_cut(\n",
    "            name=\"is_reco_electron\", \n",
    "            description=\"Tracks are assumed to be electrons (trk)\", \n",
    "            mask=is_trk_electron # has_trk_parent_electron\n",
    "        )\n",
    "        \n",
    "        # 2. Downstream tracks only through tracker\n",
    "        is_downstream = selector.is_downstream(data[\"trkfit\"])\n",
    "    \n",
    "         # trkseg-level definition, useful for plotting\n",
    "        data[\"is_downstream_in_trk\"] = is_downstream & trk_front \n",
    "    \n",
    "        # trk-level definition (the actual cut)\n",
    "        is_downstream = ak.all(~in_trk | is_downstream, axis=-1)\n",
    "        cut_manager.add_cut(\n",
    "            name=\"is_downstream\",\n",
    "            description=\"Downstream tracks only (p_z > 0 through tracker)\",\n",
    "            mask=is_downstream\n",
    "        )\n",
    "        \n",
    "        # 3. Minimum hits\n",
    "        has_hits = selector.has_n_hits(data[\"trk\"], nhits=20)\n",
    "        cut_manager.add_cut(\n",
    "            name=\"has_hits\",\n",
    "            description=\"Minimum of 20 active hits in the tracker\",\n",
    "            mask=has_hits\n",
    "        )\n",
    "    \n",
    "        if on_spill:\n",
    "            # 4. Time at tracker entrance (trk level)\n",
    "            within_t0 = ((640 < data[\"trkfit\"][\"trksegs\"][\"time\"]) & \n",
    "                         (data[\"trkfit\"][\"trksegs\"][\"time\"] < 1650))\n",
    "        \n",
    "            # trk-level definition (the actual cut)\n",
    "            within_t0 = ak.all(~trk_front | within_t0, axis=-1)\n",
    "            cut_manager.add_cut( # trk-level\n",
    "                name=\"within_t0\",\n",
    "                description=\"t0 at tracker entrance (640 < t_0 < 1650 ns)\",\n",
    "                mask=within_t0\n",
    "            )\n",
    "            \n",
    "        # 5. Track quality\n",
    "        good_trkqual = selector.select_trkqual(data[\"trk\"], quality=0.8)\n",
    "        cut_manager.add_cut(\n",
    "            name=\"good_trkqual\",\n",
    "            description=\"Track quality (quality > 0.8)\",\n",
    "            mask=good_trkqual\n",
    "        )\n",
    "        \n",
    "        # 6. Loop helix maximum radius\n",
    "        within_lhr_max = ((450 < data[\"trkfit\"][\"trksegpars_lh\"][\"maxr\"]) & \n",
    "                          (data[\"trkfit\"][\"trksegpars_lh\"][\"maxr\"] < 680)) # changed from 650\n",
    "    \n",
    "        # trk-level definition (the actual cut)\n",
    "        within_lhr_max = ak.all(~trk_front | within_lhr_max, axis=-1)\n",
    "        cut_manager.add_cut(\n",
    "            name=\"within_lhr_max\",\n",
    "            description=\"Loop helix maximum radius (450 < R_max < 680 mm)\",\n",
    "            mask=within_lhr_max\n",
    "        )\n",
    "    \n",
    "        \n",
    "        # 7. Distance from origin\n",
    "        within_d0 = (data[\"trkfit\"][\"trksegpars_lh\"][\"d0\"] < 100)\n",
    "    \n",
    "        # trk-level definition (the actual cut)\n",
    "        within_d0 = ak.all(~trk_front | within_d0, axis=-1) \n",
    "        cut_manager.add_cut(\n",
    "            name=\"within_d0\",\n",
    "            description=\"Extrapolated position (d_0 < 100 mm)\",\n",
    "            mask=within_d0\n",
    "        )\n",
    "        \n",
    "        # 8. Pitch angle\n",
    "        within_pitch_angle = ((0.5577350 < data[\"trkfit\"][\"trksegpars_lh\"][\"tanDip\"]) & \n",
    "                              (data[\"trkfit\"][\"trksegpars_lh\"][\"tanDip\"] < 1.0))\n",
    "    \n",
    "        # trk-level definition (the actual cut) \n",
    "        within_pitch_angle = ak.all(~trk_front | within_pitch_angle, axis=-1)\n",
    "        cut_manager.add_cut(\n",
    "            name=\"within_pitch_angle\",\n",
    "            description=\"Extrapolated pitch angle (0.5577350 < tan(theta_Dip) < 1.0)\",\n",
    "            mask=within_pitch_angle\n",
    "        )\n",
    "    \n",
    "        # 9. CRV veto: |dt| < 150 ns (dt = coinc time - track t0) \n",
    "        dt_threshold = 150  \n",
    "    \n",
    "        # Get min and max track t0 times for each event\n",
    "        min_trk_time = ak.min(data[\"trkfit\"][\"trksegs\"][\"time\"][trk_front], axis=-1)\n",
    "        max_trk_time = ak.max(data[\"trkfit\"][\"trksegs\"][\"time\"][trk_front], axis=-1)\n",
    "    \n",
    "        # Get min and max coincidence times for each event\n",
    "        min_coinc_time = ak.min(data[\"crv\"][\"crvcoincs.time\"], axis=-1)\n",
    "        max_coinc_time = ak.max(data[\"crv\"][\"crvcoincs.time\"], axis=-1)\n",
    "    \n",
    "        # Broadcast coincidence times to match track times structure\n",
    "        # This requires creating a cartesian product between coinc times and track times\n",
    "        # We'll use the ak.cartesian function to do this\n",
    "        \n",
    "        # First, create arrays with the right structure\n",
    "        coinc_info = ak.zip({\n",
    "            \"min_time\": min_coinc_time,\n",
    "            \"max_time\": max_coinc_time\n",
    "        })\n",
    "        \n",
    "        trk_info = ak.zip({\n",
    "            \"min_time\": min_trk_time,\n",
    "            \"max_time\": max_trk_time\n",
    "        })\n",
    "        \n",
    "        # Use cartesian product to align each coinc with each track\n",
    "        matched = ak.cartesian({\"coinc\": coinc_info, \"track\": trk_info})\n",
    "        \n",
    "        # Check if dt if within threshold\n",
    "        veto = (\n",
    "            (abs(matched[\"coinc\", \"min_time\"] - matched[\"track\", \"max_time\"]) < dt_threshold) & \n",
    "            (abs(matched[\"track\", \"min_time\"] - matched[\"coinc\", \"max_time\"]) < dt_threshold)\n",
    "        )\n",
    "\n",
    "        # Mark unvetoed events\n",
    "        data[\"unvetoed\"] = ~veto\n",
    "        \n",
    "        cut_manager.add_cut(\n",
    "            name=\"unvetoed\",\n",
    "            description=\"No veto: |dt| >= 150 ns\",\n",
    "            mask=~veto,\n",
    "            active=False\n",
    "        )\n",
    "        \n",
    "        # Return the modified data and cut manager\n",
    "        # return data, cut_manager\n",
    "\n",
    "    def apply_cuts(self, data, cut_manager): \n",
    "\n",
    "        # Make a copy \n",
    "        # Not sure if needed, but preserves the original array\n",
    "        data_cut = ak.copy(data)\n",
    "        \n",
    "        # Combine cuts\n",
    "        combined = cut_manager.combine_cuts(active_only=False) \n",
    "        data_cut[\"combined\"] = combined\n",
    "        \n",
    "        # # Select tracks\n",
    "        data_cut[\"trk\"] = data_cut[\"trk\"][combined]\n",
    "        data_cut[\"trkfit\"] = data_cut[\"trkfit\"][combined]\n",
    "        data_cut[\"trkmc\"] = data_cut[\"trkmc\"][combined]\n",
    "        \n",
    "        # Then clean up events with no tracks after cuts\n",
    "        combined = ak.any(combined, axis=-1)\n",
    "        data_cut = data_cut[combined] \n",
    "\n",
    "        return data_cut\n",
    "    \n",
    "    def create_histograms(self, data, data_cut):\n",
    "        \"\"\"Create histograms from the data before and after applying cuts\n",
    "        \n",
    "        Args:\n",
    "            data: Data before cuts\n",
    "            data_cut: Data after cuts\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary of histograms\n",
    "        \"\"\"\n",
    "            \n",
    "        # Histogram container \n",
    "        # histograms = {}\n",
    "\n",
    "        # Tools \n",
    "        selector = self.selector \n",
    "        vector = self.vector\n",
    "\n",
    "        try: \n",
    "            \n",
    "            # Data\n",
    "            at_trkent_all = selector.select_surface(data[\"trkfit\"], sid=0)\n",
    "            at_trkent_cut = selector.select_surface(data_cut[\"trkfit\"], sid=0)\n",
    "            \n",
    "            mom_all = vector.get_mag(data[\"trkfit\"][\"trksegs\"][at_trkent_all], \"mom\")\n",
    "            mom_cut = vector.get_mag(data_cut[\"trkfit\"][\"trksegs\"][at_trkent_cut], \"mom\")\n",
    "        \n",
    "            mom_all = ak.flatten(mom_all, axis=None)\n",
    "            mom_cut = ak.flatten(mom_cut, axis=None)\n",
    "    \n",
    "            # Define histograms \n",
    "    \n",
    "            # Create histogram objects to store the data\n",
    "            # Full momentum range histogram\n",
    "            hist_full_range = hist.Hist(\n",
    "                hist.axis.Regular(250, 0, 250, name=\"momentum\", label=\"Momentum [MeV/c]\"),\n",
    "                hist.axis.StrCategory([\"All events\", \"CE-like events\"], name=\"selection\", label=\"Selection\")\n",
    "            )\n",
    "            \n",
    "            # Signal region histogram (fine binning)\n",
    "            hist_signal_region = hist.Hist(\n",
    "                hist.axis.Regular(13, 103.6, 104.9, name=\"momentum\", label=\"Momentum [MeV/c]\"),\n",
    "                hist.axis.StrCategory([\"All events\", \"CE-like events\"], name=\"selection\", label=\"Selection\")\n",
    "            )\n",
    "            \n",
    "            # Fill the histograms\n",
    "    \n",
    "            # Full range histogram\n",
    "            hist_full_range.fill(momentum=mom_all, selection=np.full(len(mom_all), \"All events\"))\n",
    "            hist_full_range.fill(momentum=mom_cut, selection=np.full(len(mom_cut), \"CE-like events\"))\n",
    "            \n",
    "            # Signal region histogram\n",
    "            # For signal region, we need to filter the momenta to be within the range\n",
    "            mom_all_sig = mom_all[(mom_all >= 103.6) & (mom_all <= 104.9)]\n",
    "            mom_cut_sig = mom_cut[(mom_cut >= 103.6) & (mom_cut <= 104.9)]\n",
    "            \n",
    "            hist_signal_region.fill(momentum=mom_all_sig, selection=np.full(len(mom_all_sig), \"All events\"))\n",
    "            hist_signal_region.fill(momentum=mom_cut_sig, selection=np.full(len(mom_cut_sig), \"CE-like events\"))\n",
    "    \n",
    "            return {\n",
    "                \"Wide range\" : hist_full_range, \n",
    "                \"Signal region\": hist_signal_region\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during processing\n",
    "            print(f\"{self.print_prefix}Error filling histograms: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def execute(self, data, file_id):\n",
    "        \"\"\"Perform complete analysis on an array\n",
    "        \n",
    "        Args:\n",
    "            data: The data to analyse\n",
    "            file_id: Identifier for the file\n",
    "            \n",
    "        Returns:\n",
    "            dict: Complete analysis results\n",
    "        \"\"\"\n",
    "        # Create a fresh cut manager for this file\n",
    "\n",
    "        cut_manager = CutManager(verbosity=self.verbosity)\n",
    "        \n",
    "        # Define cuts\n",
    "\n",
    "        self.define_cuts(data, cut_manager)\n",
    "\n",
    "        # Calculate cut stats\n",
    "\n",
    "        cut_stats = cut_manager.calculate_cut_stats(data, progressive=True)\n",
    "        # cut_manager.print_cut_stats(data, active_only=False)\n",
    "\n",
    "        # Apply cuts\n",
    "\n",
    "        data_cut = self.apply_cuts(data, cut_manager)\n",
    "        \n",
    "        # Create histograms\n",
    "\n",
    "        histograms = self.create_histograms(data, data_cut)\n",
    "        \n",
    "        # Compile all results\n",
    "        results = {\n",
    "            \"file_id\": file_id,\n",
    "            \"cut_stats\": cut_stats,\n",
    "            \"filtered_data\": data_cut,\n",
    "            \"histograms\": histograms,\n",
    "        }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b06c1d-d545-44c2-97a1-d499bc947273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Skeleton] ⭐️ Skeleton init\n",
      "[Analyse] Initialised\n",
      "[MyProcessor] Initialised\n",
      "[Skeleton] ⭐️ Starting analysis\n",
      "[pyutils] ⭐️ Setting up...\n",
      "[pyutils] ✅ Ready\n",
      "[pyprocess] ⭐️ Initialised Processor:\n",
      "\tpath = 'EventNtuple/ntuple'\n",
      "\tuse_remote = True\n",
      "\tlocation = disk\n",
      "\tschema = root\n",
      "\tverbosity=1\n",
      "[pyprocess] ⭐️ Loading file list from /exp/mu2e/data/users/sgrant/mu2e_cosmic_ana/nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.0_99.txt\n",
      "[pyprocess] ✅ Successfully loaded file list\n",
      "\tPath: None\n",
      "\tCount: 100 files\n",
      "[pyprocess] ⭐️ Starting processing on 100 files with 100 threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|\u001b[32m                              \u001b[0m| 0/100 [00:00<?, ?file/s]\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create your custom processor class\n",
    "# We inherit from Skeleton - this gives us many pre-built methods without writing extra code\n",
    "class MyProcessor(Skeleton):\n",
    "    \"\"\"Your custom file processor \n",
    "    \n",
    "    This class inherits from the Skeleton base class, which provides the \n",
    "    basic structure and methods withing the Processor framework \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise your processor with specific configuration\n",
    "        \n",
    "        This method sets up all the parameters needed for this specific analysis.\n",
    "        \"\"\"\n",
    "        # Call the parent class's __init__ method first\n",
    "        # This ensures we have all the base functionality properly set up\n",
    "        super().__init__()\n",
    "\n",
    "        # Now override parameters from the Skeleton with the ones we need\n",
    "        # Data selection configuration \n",
    "        # self.defname = \"nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.root\"\n",
    "        # self.file_list_path = \"/exp/mu2e/data/users/sgrant/EventNtuple/TestFileLists/local_file_list.txt\"\n",
    "        self.file_list_path = \"/exp/mu2e/data/users/sgrant/mu2e_cosmic_ana/nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.0_99.txt\"\n",
    "        # self.file_list_path = \"/exp/mu2e/data/users/sgrant/mu2e_cosmic_ana/nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.0_24.txt\"\n",
    "        # self.file_list_path = \"/exp/mu2e/data/users/sgrant/mu2e_cosmic_ana/nts.mu2e.CosmicCRYSignalAllOffSpillTriggered-LH.MDC2020as_best_v1_3_v06_03_00.0.txt\"\n",
    "    \n",
    "        self.branches = { \n",
    "            \"evt\" : [\n",
    "                \"run\",\n",
    "                \"subrun\",\n",
    "                \"event\",\n",
    "            ],\n",
    "            \"crv\" : [\n",
    "                \"crvcoincs.time\",\n",
    "                \"crvcoincs.nHits\"\n",
    "            ],\n",
    "            \"trk\" : [\n",
    "                \"trk.nactive\", \n",
    "                \"trk.pdg\", \n",
    "                \"trkqual.valid\",\n",
    "                \"trkqual.result\"\n",
    "            ],\n",
    "            \"trkfit\" : [\n",
    "                \"trksegs\",\n",
    "                \"trksegpars_lh\"\n",
    "            ],\n",
    "            \"trkmc\" : [\n",
    "                \"trkmcsim\"\n",
    "            ]\n",
    "        }\n",
    "        self.use_remote = True # Use remote file via mdh\n",
    "        self.location = \"disk\" # File location\n",
    "        self.max_workers = 100 # Limit the number of worker threads\n",
    "\n",
    "        # Now add your own analysis-specific parameters \n",
    "        # # Histogram configuration: 1000 bins ranging from 0 to 10000\n",
    "        # self.nbins = 1000         \n",
    "        # self.xrange = (0, 10000)  \n",
    "\n",
    "        # Analysis tools\n",
    "        self.analyse = Analyse(verbosity=0)\n",
    "            \n",
    "        # Custom prefix for log messages from this processor\n",
    "        self.print_prefix = \"[MyProcessor] \"\n",
    "        print(f\"{self.print_prefix}Initialised\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # Define the core processing logic\n",
    "    # ==========================================\n",
    "    # This method overrides the parent class's process_file method\n",
    "    # It will be called automatically for each file by the execute method\n",
    "    def process_file(self, file_name): \n",
    "        \"\"\"Process a single ROOT file\n",
    "        \n",
    "        This method will be called for each file in our list.\n",
    "        It extracts data, processes it, and returns a result.\n",
    "        \n",
    "        Args:\n",
    "            file_name: Path to the ROOT file to process\n",
    "            \n",
    "        Returns:\n",
    "            A tuple containing the histogram (counts and bin edges)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create an Importer to extract data from this file\n",
    "            # This uses the configuration parameters from our class\n",
    "            \n",
    "            this_processor = Processor(\n",
    "                use_remote=self.use_remote,     # Use remote file via mdh\n",
    "                location=self.location,         # File location\n",
    "                verbosity=0                     # Reduce output in worker threads\n",
    "            )\n",
    "            \n",
    "            # Extract the data using the Importer\n",
    "            this_data = this_processor.process_data(\n",
    "                file_name = file_name, \n",
    "                branches = self.branches\n",
    "            )\n",
    "            \n",
    "            # ---- Analysis ----            \n",
    "            results = self.analyse.execute(this_data, file_name)\n",
    "\n",
    "            # Maybe results should be indexed dict instead with the file name?  \n",
    "            return results \n",
    "        \n",
    "        except Exception as e:\n",
    "            # Handle any errors that occur during processing\n",
    "            print(f\"{self.print_prefix}Error processing {file_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "# ==========================================\n",
    "# Create and run the processor\n",
    "# ==========================================\n",
    "# Create an instance of our custom processor\n",
    "my_processor = MyProcessor()\n",
    "\n",
    "# Run the processor on all files\n",
    "# The execute method comes from the Skeleton parent class\n",
    "results = my_processor.execute()\n",
    "\n",
    "# At this point, 'hists' contains the results from all processed files\n",
    "# You can now analyze, plot, or save these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f8de86-92e2-48d4-ba79-c59febb517dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(results.keys()) \n",
    "# results[0][\"cut_stats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcf047-2f8f-4b56-9df6-301a6336e639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be in cut manager \n",
    "\n",
    "def combine_cut_stats(list_of_cut_stats):\n",
    "    \"\"\"Combine cut statistics from multiple files in the simplest way possible\n",
    "    \n",
    "    Args:\n",
    "        list_of_cut_stats: List of cut statistics lists from different files\n",
    "    \n",
    "    Returns:\n",
    "        list: Combined cut statistics\n",
    "    \"\"\"\n",
    "    # Return empty list if no input\n",
    "    if not list_of_cut_stats:\n",
    "        return []\n",
    "    \n",
    "    # Use the first list as a template\n",
    "    combined_stats = []\n",
    "    for cut in list_of_cut_stats[0]:\n",
    "        # Create a copy without the mask (which we don't need)\n",
    "        cut_copy = {k: v for k, v in cut.items() if k != 'mask'}\n",
    "        # Reset the event count\n",
    "        cut_copy['events_passing'] = 0\n",
    "        combined_stats.append(cut_copy)\n",
    "    \n",
    "    # Sum up events_passing for each cut across all files\n",
    "    for file_stats in list_of_cut_stats:\n",
    "        for i, cut in enumerate(file_stats):\n",
    "            if i < len(combined_stats):  # Safety check\n",
    "                combined_stats[i]['events_passing'] += cut['events_passing']\n",
    "    \n",
    "    # Recalculate percentages\n",
    "    if combined_stats and combined_stats[0]['events_passing'] > 0:\n",
    "        total_events = combined_stats[0]['events_passing']\n",
    "        previous_events = total_events\n",
    "        \n",
    "        for i, cut in enumerate(combined_stats):\n",
    "            events = cut['events_passing']\n",
    "            \n",
    "            # Absolute percentage\n",
    "            cut['absolute_frac'] = (events / total_events) * 100.0\n",
    "            \n",
    "            # Relative percentage\n",
    "            if i == 0:  # \"No cuts\"\n",
    "                cut['relative_frac'] = 100.0\n",
    "            else:\n",
    "                prev_events = combined_stats[i-1]['events_passing']\n",
    "                cut['relative_frac'] = (events / prev_events) * 100.0 if prev_events > 0 else 0.0\n",
    "    \n",
    "    return combined_stats\n",
    "\n",
    "stats = []\n",
    "for result in results:\n",
    "    stats.append(result[\"cut_stats\"])\n",
    "    \n",
    "combined_stats = combine_cut_stats(stats)\n",
    "# cut_manager = CutManager()\n",
    "# cut_manager.print_cut_stats(stats=combined_stats, active_only=False)\n",
    "# print(combined_stats)\n",
    "# print(stats[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da33662-af30-4409-9532-146380c36b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should actually come from cut manager \n",
    "\n",
    "def print_cut_stats(data=None, stats=None, progressive=True, active_only=False):\n",
    "    \"\"\" Print cut statistics for each cut.\n",
    "    \n",
    "    Args:\n",
    "        data (awkward.Array): Data array\n",
    "        progressive (bool, optional): If True, apply cuts progressively; if False, apply each cut independently\n",
    "    \"\"\"\n",
    "\n",
    "    # Input validation\n",
    "    sources = sum(x is not None for x in [data, stats]) \n",
    "    if sources != 1:\n",
    "        print(f\"{self.print_prefix} Please provide exactly one of 'data' or 'stats'\") \n",
    "        return None\n",
    "\n",
    "    if not stats:\n",
    "        stats = calculate_cut_stats(data, progressive, active_only)\n",
    "    \n",
    "    # Print header\n",
    "    # print(f\"\\n{self.print_prefix}Cut Info:\")\n",
    "    print(\"-\" * 110)\n",
    "    header = \"{:<20} {:<10} {:<20} {:<20} {:<20} {:<30}\".format(\n",
    "        \"Cut\", \"Active\", \"Events Passing\", \"Absolute Frac. [%]\", \"Relative Frac. [%]\", \"Description\")\n",
    "    print(header)\n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    # Print each cut's statistics\n",
    "    for stat in stats:\n",
    "        row = \"{:<20} {:<10} {:<20} {:<20.2f} {:<20.2f} {:<30}\".format(\n",
    "            stat[\"name\"], \n",
    "            stat[\"active\"],\n",
    "            stat[\"events_passing\"], \n",
    "            stat[\"absolute_frac\"], \n",
    "            stat[\"relative_frac\"], \n",
    "            stat[\"description\"])\n",
    "        print(row)\n",
    "        \n",
    "    print(\"-\" * 110)\n",
    "    \n",
    "    # Print final statistics\n",
    "    if len(stats) > 1:\n",
    "        first_events = stats[0][\"events_passing\"]\n",
    "        last_events = stats[-1][\"events_passing\"]\n",
    "        overall_eff = last_events / first_events * 100 if first_events > 0 else 0\n",
    "        \n",
    "        # print(f\"{self.print_prefix}Summary: {last_events}/{first_events} events remaining ({overall_eff:.2f}%)\")\n",
    "        print(f\"Summary: {last_events}/{first_events} events remaining ({overall_eff:.2f}%)\")\n",
    "        \n",
    "print_cut_stats(stats=combined_stats, active_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aaba24-7f5d-4f70-984d-dde712bdb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_histograms(results):\n",
    "    \"\"\"Combine histograms from multiple files\n",
    "    \n",
    "    Args:\n",
    "        file_results: Dictionary with file IDs as keys and results as values\n",
    "                     Each result contains a 'histograms' dict\n",
    "    \n",
    "    Returns:\n",
    "        dict: Combined histograms\n",
    "    \"\"\"\n",
    "    combined_hists = {}\n",
    "    \n",
    "    # Check if we have results\n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    # Loop through all files\n",
    "    for result in results: # \n",
    "        # Skip if no histograms in this file\n",
    "        if 'histograms' not in result or not result['histograms']:\n",
    "            continue\n",
    "        \n",
    "        # Process each histogram type\n",
    "        for hist_name, hist_obj in result['histograms'].items():\n",
    "            if hist_name not in combined_hists:\n",
    "                # First time seeing this histogram type, initialise\n",
    "                combined_hists[hist_name] = hist_obj.copy()\n",
    "            else:\n",
    "                # Add this histogram to the accumulated one\n",
    "                combined_hists[hist_name] += hist_obj\n",
    "    \n",
    "    return combined_hists\n",
    "\n",
    "# Usage example:\n",
    "combined_histograms = combine_histograms(results)\n",
    "\n",
    "# Now you can plot them\n",
    "plt.style.use(\"/home/sgrant/pyutils-dev/EventNtuple/utils/pyutils/mu2e.mplstyle\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6.4*2, 4.8))\n",
    "\n",
    "# Plot wide range in first subplot\n",
    "if 'Wide range' in combined_histograms:\n",
    "    h_wide = combined_histograms['Wide range']\n",
    "    h_wide.plot1d(overlay='selection', ax=axes[0])\n",
    "    axes[0].set_title(\"Wide range: 0-250 MeV/c\")\n",
    "    axes[0].set_yscale('log')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot signal region in second subplot\n",
    "if 'Signal region' in combined_histograms:\n",
    "    h_signal = combined_histograms['Signal region']\n",
    "    h_signal.plot1d(overlay='selection', ax=axes[1])\n",
    "    axes[1].set_title(\"Signal region: 103.6-104.9 MeV/c\")\n",
    "    axes[1].set_yscale('log')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_ylabel(\"\")  # Remove duplicate y-label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"combined_momentum_comparison.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fff8e7-0b6a-4f79-9881-4fb46c817c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results[0])\n",
    "data_cut = {} \n",
    "for result in results: \n",
    "    this_data_cut = result[\"filtered_data\"]\n",
    "    if len(this_data_cut) == 0: \n",
    "        continue\n",
    "\n",
    "    print(result[\"file_id\"])\n",
    "    \n",
    "    if len(data_cut) == 0: \n",
    "        data_cut = this_data_cut\n",
    "    else: \n",
    "        this_data_cut = ak.concatenate(data_cut)\n",
    "        print(result[\"file_id\"])\n",
    "\n",
    "printer = Print()\n",
    "printer.print_n_events(data_cut, n_events=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f586df-408e-4c32-be9c-d4c96a82028c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-test_coffea]",
   "language": "python",
   "name": "conda-env-.conda-test_coffea-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
